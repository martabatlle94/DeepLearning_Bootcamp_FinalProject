{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scrapping: Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# Database\n",
    "import sqlalchemy as db\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re #for avoiding looking at titles with starting parenthesis\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# Logging\n",
    "from v_log import VLogger\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "batch_num = 0\n",
    "\n",
    "# Fixed batch_size\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start logging\n",
    "log = VLogger(f'Batch {batch_num}', uri_log=f\"log/WebScrap.log\", file_log_level = logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the clean database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1 Connect to the database CLEAN\n",
    "\n",
    "#Paths\n",
    "path_db_final = os.path.join(\"..\",\"data\",\"MSD\",\"clean.db\")\n",
    "path_sql_connection_db =  'sqlite:///' + path_db_final\n",
    "\n",
    "#Connect\n",
    "engine = db.create_engine(path_sql_connection_db)\n",
    "connection = engine.connect()\n",
    "\n",
    "#Log\n",
    "log.info(\"1 Connect to database...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(qq, con = connection, to_df = False):\n",
    "    res = con.execute(qq)\n",
    "    if to_df:\n",
    "        return pd.DataFrame(res.fetchall())\n",
    "    else:\n",
    "        return res.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_insert(qq, con = connection):\n",
    "    try:\n",
    "        res = con.execute(qq)\n",
    "        return res\n",
    "    except:\n",
    "        return False        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log\n",
    "log.info(\"2 Select all songs...\")\n",
    "\n",
    "# Take all songs\n",
    "df = query_db(\"SELECT * FROM youtube_url\", to_df=True)\n",
    "df.columns = [\"track_id\",\"title\",\"artist_id\",\"yt_url\",\"duration\",\"year\",\"artist_name\",\"artist_iid\"]\n",
    "\n",
    "#Log\n",
    "log.info(\"2 Select all songs... (Completed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create useful fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the title and the artist name\n",
    "df[\"tit_art\"] = df[\"title\"] + \" \" + df[\"artist_name\"]\n",
    "df[\"tit_art\"] = df[\"tit_art\"].str.lower()\n",
    "df[\"title\"] = df[\"title\"].str.lower()\n",
    "df[\"artist_name\"] = df[\"artist_name\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. WebScrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list for every artist_iid, we create a tuple tit_art & track_id, so that we can iterate for that artist through all of the songs in the MSD and see if any of the track id coincides all the words in the tit_art with the words in the youtube title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_clean_title(titart, prog):\n",
    "    \"\"\"\n",
    "    Cleans the title of a MSD song to avoid () or [] or any special character\n",
    "    \"\"\"\n",
    "    # CLEANING TITLE\n",
    "    words_list = titart.split(\" \")\n",
    "    # we want to avoid words that don't start with a character\n",
    "    words_set = set()\n",
    "    for ww in words_list:\n",
    "        result = prog.match(ww)\n",
    "        if result is not None: # avoid starting word with parenthesis\n",
    "            if '\\\\' not in ww: #avoid non-coded characters \\x19\n",
    "                words_set.add(ww)\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MSD_title_match_yt_title(artist_name, dict_URL_artist, tuple_titart_trackid, prog):\n",
    "    \"\"\"\n",
    "    Get the artist_name and the list of tuples of songs-songID for that artist in MSD\n",
    "    Returns: a list of matchins and the list of non-mtched songs-songID\n",
    "    The list of matchings includes songID - youtubeHREF\n",
    "    \"\"\"\n",
    "    # Lista final\n",
    "    list_matchings = []\n",
    "\n",
    "    # Loop over each yt song\n",
    "    for yts_tit, yts_href in dict_URL_artist[artist_name]:\n",
    "        if yts_href is None:\n",
    "            continue #if it is a playlist\n",
    "\n",
    "        # Split into words the yt_title\n",
    "        words_yt_tit = list(yts_tit.split(\" \"))\n",
    "        words_yt_tit = [stt.lower() for stt in words_yt_tit] # convert strings to lower\n",
    "        words_yt_tit = set(words_yt_tit)\n",
    "\n",
    "        # Loop over all the songs in MSD for that artist_name \n",
    "        #(droping in case of we find a match the song from the MSD list)\n",
    "        for ii, tup in enumerate(tuple_titart_trackid):\n",
    "\n",
    "            # Get the MSD TitArt and the Track ID\n",
    "            msd_titart, msd_tid = tup\n",
    "            \n",
    "            # Avoid songs that are named totally equal as their artist_name\n",
    "            if msd_titart == artist_name:\n",
    "                continue\n",
    "\n",
    "            # CLEAN title of MSD\n",
    "            set_titart_msd = fun_clean_title(msd_titart, prog)\n",
    "            length_titart_msd = len(set_titart_msd)\n",
    "\n",
    "            # Make the intersection\n",
    "            intersect_yt_msd_tit = set_titart_msd.intersection(words_yt_tit)\n",
    "\n",
    "            # See if all the words in MSD are present in that yt_title\n",
    "            if len(intersect_yt_msd_tit) == length_titart_msd:\n",
    "\n",
    "                # Add the track ID and the href\n",
    "                list_matchings.append((msd_tid,yts_href))\n",
    "\n",
    "                # Delete the song ffrom the tuple_titar_trackid\n",
    "                del tuple_titart_trackid[ii]\n",
    "\n",
    "                #Get out of the loop since we have found a match for that yt song\n",
    "                break\n",
    "    return list_matchings, tuple_titart_trackid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WebScrapperYoutube(df_WS):\n",
    "    \"\"\"\n",
    "    Performs the WebScrapping\n",
    "    \"\"\"\n",
    "    #Log\n",
    "    log.info(\"5.1 Launching Chrome...\")\n",
    "\n",
    "    # Create a dictionary of all the searches with the artist and the titles-href as tuples in that dictionary\n",
    "    browser = webdriver.Chrome()\n",
    "\n",
    "    # OUTPUT: Dict for the matching songs and the non-matching\n",
    "    dict_matching = dict()\n",
    "    dict_non_matching = dict()\n",
    "\n",
    "    #Regular expression to avoid strings in MSD initiated with NOT letters\n",
    "    prog = re.compile(\"^[A-Za-z]\") \n",
    "\n",
    "    # Loop for each batch of artists\n",
    "    for ii,row in tqdm.tqdm(df_WS.iterrows()):\n",
    "\n",
    "        # Get all the list of tracks for that artist\n",
    "        artist_iid, artist_name, tuple_titart_trackid = row\n",
    "        \n",
    "        #Log\n",
    "        log.info(f\"5.1 Scrapping Artist {artist_name}...\")\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # Dicts for each artist\n",
    "        dict_URL_artist = dict()\n",
    "\n",
    "        #Output\n",
    "        dict_matching[artist_name] = dict()\n",
    "        dict_non_matching[artist_name] = dict()\n",
    "        # ----------------------------------------------\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        #Search that artist on youtube\n",
    "        browser.get(f\"https://www.youtube.com/results?search_query={artist_name}\")\n",
    "\n",
    "        # List all the elements in video-title\n",
    "        vid_title_elems = browser.find_elements_by_id('video-title')\n",
    "\n",
    "        #Save youtube results for that artists\n",
    "        dict_URL_artist[artist_name] = list()\n",
    "\n",
    "        # Save videos and their URL as tuple\n",
    "        for vte in vid_title_elems:\n",
    "            try:\n",
    "                yt_title =  vte.get_attribute(\"title\")\n",
    "                yt_href  =  vte.get_attribute('href')\n",
    "                dict_URL_artist[artist_name].append((yt_title, yt_href))\n",
    "            except:\n",
    "                continue\n",
    "        # ----------------------------------------------\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        try:\n",
    "            # Run the title comparator function\n",
    "            match_LIST, notmatch_LIT = get_MSD_title_match_yt_title(artist_name, dict_URL_artist, tuple_titart_trackid, prog)\n",
    "            # ----------------------------------------------\n",
    "            dict_matching[artist_name] = match_LIST\n",
    "            dict_non_matching[artist_name] = notmatch_LIT\n",
    "        except:\n",
    "            #Log\n",
    "            log.info(f\"5.2 Exception in Artist {artist_name}...\")\n",
    "            dict_non_matching[artist_name] = tuple_titart_trackid\n",
    "\n",
    "    browser.close()        \n",
    "    return dict_matching, dict_non_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_dmatch(dmatch, batch_num):\n",
    "    query_insert_string = \"insert into match values \"\n",
    "    for arr, tup in dmatch.items():\n",
    "        for song in tup:\n",
    "            track_id, url = song\n",
    "            query_insert_string += f\"('{track_id}', '{url}', '{batch_num}'),\"\n",
    "    query_insert_string = query_insert_string[:-1]\n",
    "    query_insert_string = query_insert_string + \";\"\n",
    "    \n",
    "    # Insert\n",
    "    try:\n",
    "        res = query_insert(query_insert_string)\n",
    "        return res, query_insert_string\n",
    "    except:\n",
    "        return False, query_insert_string\n",
    "\n",
    "def insert_dnonmatch(dnonmatch, batch_num):\n",
    "    query_insert_string = \"insert into nonmatch values \"\n",
    "    for arr, tup in dnonmatch.items():\n",
    "        for song in tup:\n",
    "            yt_query, track_id = song\n",
    "            if \"'\" in yt_query:\n",
    "                yt_query = yt_query.replace(\"'\",\" \")\n",
    "            query_insert_string += f\"('{track_id}', '{yt_query}', '{batch_num}'),\"\n",
    "    query_insert_string = query_insert_string[:-1]\n",
    "    query_insert_string = query_insert_string + \";\"\n",
    "    \n",
    "     # Insert\n",
    "    try:\n",
    "        res = query_insert(query_insert_string)\n",
    "        return res, query_insert_string\n",
    "    except:\n",
    "        return False, query_insert_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 -  Songs for the artists in the same register as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log\n",
    "log.info(\"3 Create list of songs for each artist...\")\n",
    "\n",
    "# Create a unique register for each artist to store all of his songs\n",
    "df_list_tit_art = df.groupby(['artist_iid','artist_name'])[['tit_art', \"track_id\"]].apply(lambda x: x.values.tolist()) \\\n",
    "    .reset_index(name = \"tuple_titart_trackid\").sort_values(['artist_iid']).set_index(\"artist_iid\")\n",
    "\n",
    "#Log\n",
    "log.info(\"3 Create list of songs for each artist... (Completed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch as batchs the artists queries to youtube (Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log\n",
    "log.info(\"4 Select artists in batch...\")\n",
    "\n",
    "# Number of all artists\n",
    "num_queries = df_list_tit_art.shape[0]\n",
    "\n",
    "#List of artist_iid that will take part in that batch\n",
    "l_queries = df_list_tit_art.index.values\n",
    "\n",
    "# Select the range of artist_iid for that batch\n",
    "idx_artist_iid_batch =  l_queries[batch_num*batch_size : (batch_num+1)*batch_size]\n",
    "\n",
    "# Select the dataframe registers for that batch\n",
    "df_WebScrapping = df_list_tit_art.loc[idx_artist_iid_batch,:].reset_index()\n",
    "\n",
    "#Log\n",
    "log.info(\"4 Select artists in batch... (Completed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfcea8a450e406ebe32e64b2fd2e9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Log\n",
    "log.info(\"5 Scrapping...\")\n",
    "\n",
    "dmatch, dnonmatch = WebScrapperYoutube(df_WebScrapping)\n",
    "\n",
    "#Log\n",
    "log.info(\"5 Scrapping... (Completed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log\n",
    "log.info(\"6 Inserting into match and nonmatch tables...\")\n",
    "\n",
    "# Run the queries to insert info\n",
    "resmatch, qqmatch = insert_dmatch(dmatch, batch_num)\n",
    "resnonmatch, qqnonmatch = insert_dnonmatch(dnonmatch, batch_num)\n",
    "\n",
    "# Report errors in the logging\n",
    "if resmatch is False:\n",
    "    log.info(f\"6.1 EROR INSERTING query in match: {qqmatch}...\")\n",
    "    \n",
    "if resnonmatch is False:\n",
    "    log.info(f\"6.1 EROR INSERTING query in nonmatch: {qqnonmatch}...\")\n",
    "    \n",
    "if resmatch:\n",
    "    if resnonmatch:\n",
    "        log.info(f\"7 Succesfully scrapped Batch {batch_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
